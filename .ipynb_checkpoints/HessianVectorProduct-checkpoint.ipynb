{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Hessian-vector product\n",
    "\n",
    "Often times we need Hessian-vector products in numerical optimization. A nice family of examples of using Hessian-vector products is Krylov subspace based iterative algorithms such as Conjugate Gradient, Lanczos method, and Power method. In deep learning literature, Krylov subspace is often used in the context of 2nd-order optimization. Please refer to [Deep learning via Hessian-free optimization](http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf), [Krylov Subspace Descent for Deep Learning](http://www1.icsi.berkeley.edu/~vinyals/Files/vinyals_aistats12.pdf), and [Identifying and attacking the saddle point problem in\n",
    "high-dimensional non-convex optimization](https://arxiv.org/pdf/1406.2572v1.pdf) for more detail. \n",
    "\n",
    "There are three ways to compute Hessian-vector products that we can try in TensorFlow.\n",
    "\n",
    "1. Compute Hessian directly and multiply the result with a vector.\n",
    "2. Use finite differences to get Hessian and multiply the result with a vector. (used in Marten's paper)\n",
    "3. Use Pearlmutter trick. (will explain later)\n",
    "\n",
    "In this post, I will compare these methods in terms of numerical stability and computational speed. \n",
    "I'm mainly interested in how well the finite differences would work when the parameter space gets big / when the condition number of Hessian becomes large, compared to Pearlmutter trick. \n",
    "\n",
    "The Method 1 is just for illustrative purpose. In practice, one should not compute Hessian directly because it's too much computation. \n",
    "\n",
    "The reason why I included method 2 is just for reference. I've tried to compute Hessian-vector product once in Torch7, which doesn't have an easy way to do the method 3. For TensorFlow / Theano users, Method 3 should suffice.   \n",
    "\n",
    "I made a TensorFlow function to compute Hessians for multilayer perceptron before, so I'll reuse it to try out the method 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "#from datasets import dataset_utils\n",
    "\n",
    "# Main slim library\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def method1(v):\n",
    "    H_1 = getHessian()# Did I make this function before?\n",
    "    return np.dot(H_1, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keras experiments: how graph is created\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "def testKeras(g):\n",
    "    with g.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            K.set_session(sess)\n",
    "            \n",
    "            # this placeholder will contain our input digits, as flat vectors\n",
    "            # img = tf.placeholder(tf.float32, shape=(None, 784))\n",
    "            \n",
    "            print('success')\n",
    "            # from keras import backend as K\n",
    "            # from keras.models import Sequential\n",
    "            # from keras.layers import Dense, Activation\n",
    "            model = Sequential()\n",
    "            input_size = 64\n",
    "            # c = tf.constant(4.0)\n",
    "            input_ph = K.placeholder(shape=(None, input_size))\n",
    "            model.add(Dense(output_dim=25, input_dim=input_size))\n",
    "            model.add(Activation(\"sigmoid\"))\n",
    "            model.add(Dense(output_dim=64, input_dim=25))\n",
    "            model.add(Activation(\"sigmoid\")) \n",
    "            \n",
    "            # assert model.graph is g\n",
    "            # print(model.summary())\n",
    "            g2 = tf.get_default_graph()\n",
    "            assert g is g2\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "testKeras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def method2(g, v):\n",
    "    with g.as_default():\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def method2(v):\n",
    "    with tf.Graph().as_default():\n",
    "        inputs = \n",
    "        targets = \n",
    "        # Compute finite differences of gradients\n",
    "        output, end_points = build_MLP(inputs, n_hidden=10)\n",
    "        # loss = slim.losses.sum_of_squares(predictions, targets) # Slim way\n",
    "        # The total loss is the uers's loss plus any regularization losses.\n",
    "        # total_loss = slim.losses.get_total_loss() # Slim way\n",
    "        loss = tf.reduce_mean(tf.square(output - targets))\n",
    "        grad = tf.gradients(loss) # make sure that this returns a vector, not a list of gradient and something else.\n",
    "        loss_eps = tf.add(loss, eps)\n",
    "        grad_eps = tf.gradients(loss_eps) # make sure that this returns a vector, not a list of gradient and something else.\n",
    "        Hv = tf.div(grad - grad_eps, eps)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            print(sess.run(Hv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_MLP(inputs, n_hidden=5):\n",
    "    end_points = {}\n",
    "    with slim.arg_scope([slim.fully_connected], \n",
    "                        activation_fn=tf.nn.relu,\n",
    "                        weights_regularizer=slim.l2_regularizer(0.01)):\n",
    "        net = slim.fully_connected(inputs, n_hidden, scope=\"fc1\")\n",
    "        end_points[\"fc1\"] = net \n",
    "        output = slim.fully_connected(net, n_output, activation_fn=None, scope=\"output\")\n",
    "        end_points[\"output\"] = output\n",
    "        \n",
    "        return output, end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 : Pearlmutter trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearlmutter trick is a way to compute $Hv$ in an efficient way using backpropagation. The basic idea is to take the gradient of ----\n",
    "$$\n",
    "\\nabla_{\\theta}(g^T v) = H^T v  = Hv \n",
    "$$\n",
    ", where g is a gradient of the objective funtion with respect to parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-5bc8f0f6582b>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-5bc8f0f6582b>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    inputs =\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def method3(v):\n",
    "    with tf.Graph().as_default():\n",
    "        inputs = \n",
    "        targets = \n",
    "        output, end_points = build_MLP(inputs, n_hidden=10)\n",
    "        loss = tf.reduce_mean(tf.square(output - targets))\n",
    "        grad = tf.gradients(loss) # make sure that this returns a vector, not a list of gradient and something else.\n",
    "        Hv = tf.gradients(tf.mul(grad, v))\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            print(sess.run(Hv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Condition number "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condition number captures sensitivity of a system, which has inputs and outputs. It measures how outputs will change with respect to change in the inputs. Many problems **(examples?)** can be reduced to finding $x$ s.t. $Ax = b$. In this case, $b$ is the input and $x$ is the solution we want, and $A$ represents the structure of the problem or system you are interested in. There are a number of types of condition number depending on how to measure the \"sensitivity\" of the system. Let us focus on the case where $A$ is a symmetric matrix because then the definition of condition number is simple: \n",
    "\n",
    "---\n",
    "\n",
    "Condition number of a symmetric matrix $A$ is defined as \n",
    "$$ \n",
    "\\frac{|\\lambda_{max}|}{|\\lambda_{min}|} \n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "where $\\lambda_{max / min}$ is the maximum / minimum eigenvalue of the matrix $A$. For those who are interested to know why it is so, please refer to my other blog post on [induced matrix norm](). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use a simple quadratic function for our objective funtion because it's easier to control the condition number of the Hessian; we can control the condition number of a matrix through SVD by modifying the resulting eigenvalues of the diagonal matrix returned by SVD.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any quadratic function s.t. $f(x) = \\frac{1}{2} x^T A x + b^T x + c$, the Hessian of $f(x)$ is $A$ when $A$ is symmetric. So we first create a random symmetric matrix like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createRandomSymmMatrix(n_dim, condition_num): \n",
    "    random_mat = np.random.random(n_dim*n_dim).reshape(n_dim, n_dim) \n",
    "    random_mat = np.dot(np.transpose(random_mat), random_mat) # For any X, X^T X is always symmetric. \n",
    "    U,S,Vt = np.linalg.svd(random_mat) # Do svd to get diagonal elements. Note that S is a vector. \n",
    "    S = np.repeat(1.0, n_dim) # Replace the diagonal elements with a vector of 1s. \n",
    "    S[-1] = 1.0/condition_num # Let the last element of S be very small. This will determine condition number. \n",
    "    return np.dot(np.dot(U, np.diag(S)), Vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.83938984,  0.2334726 ],\n",
       "       [ 0.2334726 ,  0.66061016]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createRandomSymmMatrix(2, 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability comparison\n",
    "\n",
    "Now, we will compare Method2 and Method3 in terms of the stability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Finite differences  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute $Hv$ using finite differences, we use the following identity:\n",
    "\n",
    "$$\n",
    "Hv = \\lim_{\\epsilon \\rightarrow 0} \\frac{\\nabla f(\\theta - \\epsilon v) - \\nabla f(\\theta + \\epsilon v)}{2\\epsilon}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def method2_c(v, inputs, eps):\n",
    "    g2 = tf.Graph()\n",
    "    with g2.as_default():\n",
    "        # how to perturb weights\n",
    "        with g2.name_scope(\"grad1\") as scope:\n",
    "            output1 = build_quadratic(inputs, eps=-eps, v=v)  \n",
    "            loss1 = tf.reduce_sum(output1 - 0)\n",
    "            tvars1 = tf.get_collection(tf.GraphKeys.VARIABLES, scope=scope) \n",
    "            for i in tvars1: print(i.name)\n",
    "            grad1 = tf.gradients(loss1, tvars1)[0]\n",
    "        with g2.name_scope(\"grad2\") as scope:\n",
    "            output2 = build_quadratic(inputs, eps=eps, v=v)  \n",
    "            loss2 = tf.reduce_sum(output2 - 0) \n",
    "            tvars2 = tf.get_collection(tf.GraphKeys.VARIABLES, scope=scope)\n",
    "            for i in tvars2: print(i.name)\n",
    "            grad2 = tf.gradients(loss2, tvars2)[0]\n",
    "        Hv = 0.5*(grad1-grad2) / eps \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            return sess.run(Hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199.99999999999687"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.cond(inputs['A']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad1/Variable:0\n",
      "grad1/Variable_1:0\n",
      "grad2/Variable:0\n",
      "grad2/Variable_1:0\n",
      "89.4427190907\n",
      "5.51851954553e-11\n"
     ]
    }
   ],
   "source": [
    "n_dim = 2000\n",
    "condition_num = 200\n",
    "vec = np.repeat(1.0, n_dim).reshape([n_dim, 1])\n",
    "inputs = createInputs(n_dim, condition_num) \n",
    "eps = 1e-5 # what value should I use \n",
    "Hv_method2 = method2_c(vec, inputs, eps) \n",
    "trueHv = np.dot(np.transpose(inputs['A']), vec) # true value\n",
    "print(np.linalg.norm(Hv_method2 - trueHv))\n",
    "Hv_method3 = method3_c(vec, inputs, n_dim, condition_num)\n",
    "print(np.linalg.norm(Hv_method3 - trueHv)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Hv_method3 = method3_c(vec, inputs, n_dim, n_cond_num) \n",
    "print(np.dot(np.transpose(inputs['A']), vec)) # true value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Pearlmutter Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createInputs(n_dim, condition_num):\n",
    "    inputs = dict() \n",
    "    inputs[\"A\"] = createRandomSymmMatrix(n_dim, condition_num)\n",
    "    inputs[\"b\"] = np.random.random(n_dim).reshape(n_dim, 1)\n",
    "    inputs[\"c\"] = np.random.random(1)\n",
    "    return inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def method3_c(v, inputs, n_dim, condition_num):\n",
    "    with tf.Graph().as_default():\n",
    "        output = build_quadratic(inputs, eps=0, v=v)  \n",
    "        loss = tf.reduce_sum(output - 0)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grad = tf.gradients(loss, tvars)[0]\n",
    "        # Pearlmutter Trick\n",
    "        Hv = tf.gradients(tf.matmul(tf.transpose(grad), v), tvars)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            #print(sess.run(grad).shape)\n",
    "            #print(sess.run(tf.matmul(tf.transpose(grad), v)))\n",
    "            #print(sess.run(Hv))\n",
    "            return sess.run(Hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.87837996 -0.15390476  0.04465794  0.14183629 -0.01483392]\n",
      " [-0.15390476  0.80524034  0.05651264  0.17948753 -0.01877167]\n",
      " [ 0.04465794  0.05651264  0.98360195 -0.05208119  0.0054469 ]\n",
      " [ 0.14183629  0.17948753 -0.05208119  0.83458703  0.01729968]\n",
      " [-0.01483392 -0.01877167  0.0054469   0.01729968  0.99819072]]\n",
      "(5, 1)\n",
      "[[ 8.28377084]]\n",
      "[array([[ 0.8961355 ],\n",
      "       [ 0.86856408],\n",
      "       [ 1.03813824],\n",
      "       [ 1.12112934],\n",
      "       [ 0.98733171]]), array([[ 0.8961355 ],\n",
      "       [ 0.86856408],\n",
      "       [ 1.03813824],\n",
      "       [ 1.12112934],\n",
      "       [ 0.98733171]])]\n",
      "[[ 0.8961355 ]\n",
      " [ 0.86856408]\n",
      " [ 1.03813824]\n",
      " [ 1.12112934]\n",
      " [ 0.98733171]]\n"
     ]
    }
   ],
   "source": [
    "n_dim = 5\n",
    "condition_num = 2\n",
    "vec = np.repeat(1.0, n_dim).reshape([n_dim, 1])\n",
    "inputs = createInputs(n_dim, condition_num)\n",
    "Hv_method3 = method3_c(vec, inputs, n_dim, n_cond_num) \n",
    "print(np.dot(np.transpose(inputs['A']), vec)) # true value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_quadratic(inputs, eps, v):\n",
    "    A = inputs[\"A\"]\n",
    "    # print(A)\n",
    "    b = inputs[\"b\"]\n",
    "    c = inputs[\"c\"]\n",
    "    n_dim, _ = np.shape(A)\n",
    "    \n",
    "    x_ = tf.Variable(np.float64(np.repeat(1.0,n_dim).reshape(n_dim,1)))\n",
    "    assert v.shape == (n_dim, 1)\n",
    "    epsv = tf.Variable(np.float64(eps*v))\n",
    "    x = x_ + epsv\n",
    "    \n",
    "    # Construct the computational graph for quadratic function: f(x) = 1/2 * x^t A x + b^t x + c\n",
    "    output = 0.5 * tf.matmul(tf.matmul(tf.transpose(x), A), x) + tf.matmul(tf.transpose(b), x) + c\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.000000000000005"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.cond(createMatrix(3,10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_dim = 20\n",
    "condition_num_list = \n",
    "vec = np.repeat(1.0, n_dim).reshape([n_dim, 1])\n",
    "# inputs = createInputs(n_dim, condition_num)\n",
    "eps = 1e-5 # what value should I use \n",
    "Hv_method3 = method2_c(vec, inputs, eps) \n",
    "print(np.dot(np.transpose(inputs['A']), vec)) # true value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reference\n",
    "\n",
    "Next, let's compare the result of the above function with a method using finite differences. \n",
    "\n",
    "(By the way, there is a way to compute Hessian-vector product using only gradients, which is called Pearlmutter trick in neural network literature. Please refer to https://cswhjiang.github.io/2015/10/13/Roperator/ for more details.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
